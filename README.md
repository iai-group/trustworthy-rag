# GINGER: Grounded Information Nugget-Based Generation of Responses

[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## Summary

As conversational search replaces traditional result pages, users increasingly see short, synthesized answers without insight into underlying retrieval and generation mechanisms. This study investigates how explanations affect user trust in responses generated by Retrieval-Augmented Generation (RAG) systems and extended with three post-hoc explanation types: (1) source attribution via URLs, (2) factual grounding through linked passages, and (3) information coverage highlighting omitted aspects. Via a user study with 30 information-seeking queries, we investigate user trust preferences when presented with responses supported or not supported by explanations. Results show that users’ trust judgments do not always align with the objective reliability of responses. While source attribution most effectively increases user trust, especially for factual queries, users’ prior knowledge, presentation of the information in the response, as well as information breadth and level of detail strongly shape their judgments. 

## Explainable Response Generation
The response generation and explanation modules extend the [**GINGER**](https://github.com/iai-group/ginger-response-generation) pipeline.  
A new **nugget-based explanation generator** produces post-hoc explanations targeting three response dimensions:
- **Source Attribution**
- **Factual Grounding**
- **Information Coverage**

All related scripts are located in [`code/`](code/), with modifications described in its [README](code/README.md).  
The only modified file from the original GINGER pipeline is the `summarizer.py`.

---

## Data
All input data, generated responses, annotations, and user study results are available in the [`data/`](data/) directory (see [README](data/README.md) for details).  
The data include:
- 30 information-seeking queries from **TREC CAsT '22** with automatically generated responses and explanations produced with the GINGER response generation pipeline
- Data from the qualification task used to recruit and screen crowd workers
- Data collected in the main user study, where participants compared system-generated responses of various quality, with and without explanations

## User Study Design

The user study was conducted on **Amazon Mechanical Turk (MTurk)** and consisted of:
1. A **qualification task** assessing participants’ comprehension.
2. The **main user trust study** comparing reliable vs. unreliable responses across explanation types.

Screenshots of both interfaces are available in [`study_design_screenshots/`](study_design_screenshots/) (see [README](study_design_screenshots/README.md)).

## Results
The processed outputs and aggregated statistics for the data collected in the user study can be found in:
- [`data/user_study/output_processed.csv`](data/user_study/output_processed.csv)
- [`data/user_study/trust_scores_distribution.pdf`](data/user_study/trust_scores_distribution.pdf)

Key findings:
- User trust does not always align with objective quality of the response.
- Source attribution most strongly boosts trust, especially for factual queries.
- Clarity, detail, and user expertise play major roles in shaping trust.

**All collected data, generated responses, and annotations are publicly available in this repository.**

## Citation

TODO

## Contact

TODO